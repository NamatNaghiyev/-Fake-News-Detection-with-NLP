import streamlit as st
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import re
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split
import pandas as pd
import os

# NLTK resurslarÄ±nÄ± yÃ¼klÉ™yirik
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt')

# Lemmatizer obyektini yaradÄ±rÄ±q
lemmatizer = WordNetLemmatizer()

# Stopwords siyahÄ±sÄ±nÄ± yÃ¼klÉ™yirik
stop_words = set(stopwords.words('english'))

# MÉ™tnin tÉ™mizlÉ™nmÉ™si funksiyasÄ±
def clean_text(text):
    # KiÃ§ik hÉ™rflÉ™rÉ™ Ã§eviririk
    text = text.lower()

    # XÃ¼susi simvollarÄ±, rÉ™qÉ™mlÉ™ri vÉ™ s. silirik
    text = re.sub(r'\W', ' ', text)

    # TÉ™k sÃ¶zlÉ™rÉ™ ayÄ±rÄ±rÄ±q (tokenization)
    words = text.split()

    # Stopwords-lÉ™ri aradan qaldÄ±rÄ±rÄ±q vÉ™ lemmatizasiya edirik
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]

    # TÉ™mizlÉ™nmiÅŸ mÉ™tni birlÉ™ÅŸdiririk
    return ' '.join(words)

# BaÅŸlÄ±q
st.title("ğŸ“° Fake News Classifier")

st.markdown("""
    Bu tÉ™tbiq **"Fake News"** vÉ™ **"Real News"** mÉ™tni tÉ™yin edir.  
    AÅŸaÄŸÄ±ya bir xÉ™bÉ™r mÉ™tnini yaz vÉ™ gÃ¶rÉ™k sÉ™nÉ™ nÉ™ cavab verir!
""")

# Fayl yollarÄ±nÄ± istifadiÃ§idÉ™n almaq
true_path = st.text_input("True News CSV faylÄ±nÄ±n yolu:", r'True.csv.zip')
fake_path = st.text_input("Fake News CSV faylÄ±nÄ±n yolu:", r'Fake.csv.zip')

# Fayllar varsa iÅŸlÉ™yir
if os.path.exists(true_path) and os.path.exists(fake_path):
    # VerilÉ™nlÉ™ri yÃ¼klÉ™yirik
    true_news = pd.read_csv(true_path)
    fake_news = pd.read_csv(fake_path)

    true_news['label'] = 'real'
    fake_news['label'] = 'fake'

    df = pd.concat([true_news, fake_news], ignore_index=True)

    # MÉ™tnlÉ™ri tÉ™mizlÉ™yirik
    df['clean_text'] = df['text'].apply(clean_text)

    # Tokenizer yaradÄ±lÄ±r
    tokenizer = Tokenizer(num_words=5000)
    tokenizer.fit_on_texts(df['clean_text'])

    X = tokenizer.texts_to_sequences(df['clean_text'])
    X = pad_sequences(X, maxlen=500)

    y = df['label'].apply(lambda x: 1 if x == 'fake' else 0).values

    # MÉ™lumatÄ± tÉ™lim vÉ™ test dÉ™stÉ™lÉ™rinÉ™ bÃ¶lÃ¼rÃ¼k
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    # Model yaradÄ±lÄ±r
    model = tf.keras.Sequential([
        tf.keras.layers.Dense(64, activation='relu', input_shape=(X.shape[1],)),
        tf.keras.layers.Dense(1, activation='sigmoid')
    ])

    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    with st.spinner('Model tÉ™lim olunur... biraz sÉ™birli ol... ğŸ”¥'):
        model.fit(X_train, y_train, epochs=5, batch_size=64, validation_data=(X_test, y_test))
    
    st.success('Model hazÄ±rdÄ±! ğŸ¯')

    # Ä°stifadÉ™Ã§idÉ™n mÉ™tni almaq
    user_input = st.text_area("XÉ™bÉ™r mÉ™tnini daxil edin:")

    if st.button("TÉ™snif et"):
        if user_input:
            # MÉ™tni tÉ™mizlÉ™yirik vÉ™ modelin tÉ™xminini alÄ±rÄ±q
            cleaned_text = clean_text(user_input)
            sequence = tokenizer.texts_to_sequences([cleaned_text])
            padded_sequence = pad_sequences(sequence, maxlen=500)
            prediction = model.predict(padded_sequence)

            # NÉ™ticÉ™ni gÃ¶stÉ™ririk
            if prediction > 0.5:
                st.error("XÉ™bÉ™r: **FAKE** âŒ")
            else:
                st.success("XÉ™bÉ™r: **REAL** âœ…")
        else:
            st.warning("XahiÅŸ edirik, mÉ™tn daxil edin.")
else:
    st.warning("CSV fayllarÄ±nÄ± doÄŸru gÃ¶stÉ™r. Yol sÉ™hvdir, dÃ¼z elÉ™!")

